---
author: Raza Habib
category: articles
source: reader
date: 2025-11-18
URL: https://humanloop.com/blog/ga-announcement
---
Raza Habib #reader

## Summary
Today we’re excited to open access to Humanloop for all enterprises building AI products with LLMs

## Highlights
Evals are the LLM equivalent of unit-tests — they eliminate guesswork out of AI engineering, establish quality gates for deployment, prevent regressions, and provide confidence in your AI application before it reaches production. ([View Highlight](https://read.readwise.io/read/01jgynfpnj2gdq07sxqzmjbz5w))

Building effective AI products with LLMs requires seamless collaboration between technical teams and domain experts. At Duolingo, linguists refine prompts to create engaging learning experiences. At Gusto, the customer support team directly contributes to evaluations. And at Filevine, legal professionals drive the development of AI features tailored to their industry. ([View Highlight](https://read.readwise.io/read/01jgynh8gpqedhm458qha0zyr5))

To support complex AI systems, we’ve designed a minimalist system of new abstractions that will allow you to adopt AI agents and futureproof your applications. It consists of a file system with 4 core types: “[**prompts**](https://humanloop.com/docs/explanation/prompts)”, “[**tools**](https://humanloop.com/docs/explanation/tools)”, “[**evaluators**](https://humanloop.com/docs/explanation/evaluators)” and “[**flows**](https://humanloop.com/docs/explanation/flows)”. ([View Highlight](https://read.readwise.io/read/01jgynkq63fmddn0qftrbvczxk))

