---
author: Steve Newman
category: articles
source: reader
date: 2025-11-18
URL: https://secondthoughts.ai/p/ai-coding-slowdown
---
Steve Newman #reader

## Summary
Study Shows That Even Experienced Developers Dramatically Overestimate Gains

## Highlights
METR performed a rigorous study ([blog post](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/), [full paper](https://metr.org/Early_2025_AI_Experienced_OS_Devs_Study.pdf)) to measure the productivity gain provided by AI tools for experienced developers working on mature projects. The results are surprising everyone: a 19 percent **decrease** in productivity. Even the study participants themselves were surprised: they estimated that AI had **increased** their productivity by 20 percent. If you take away just one thing from this study, it should probably be this: when people report that AI has accelerated their work, they might be wrong!
[](https://substackcdn.com/image/fetch/$s_!53MT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9797fd06-ad35-4503-88f7-952e362b19a7_2562x1540.png) ([View Highlight](https://read.readwise.io/read/01jzzchvrex00sw2k9txbpc3p2))

The only significant way in which the study design falls short of the scientific gold standard is that it was not *blinded:* once work began, both the participants and the researchers knew whether AI tools were being used. This is of course unavoidable; there is no reasonable way of providing a “placebo” coding assistant. ([View Highlight](https://read.readwise.io/read/01jzzc15mnbcz9s1qea2ps3th0))

**The John Henry Effect**: perhaps the developers were motivated to “beat the machine”, working extra-hard on AI Disallowed tasks. If this were the case, you might expect to see the effect taper off over the course of the study, as the excitement and novelty wear off – recall that subjects performed an average of 15 tasks of 1-2 hours each. No such tapering was observed. ([View Highlight](https://read.readwise.io/read/01jzzc2aes91nwa1fbwjbtap2f))

**Underuse of AI**. Perhaps developers weren’t using AI tools even when allowed? However, this could only explain a lack of productivity gain; it can’t explain a loss. And exit interviews and analysis of screen recordings both showed substantial use of AI (84% of screen recordings for AI Allowed tasks showed at least some use of AI). ([View Highlight](https://read.readwise.io/read/01jzzc2hyaz005n249mw6b7n57))

**Difference in thoroughness**. Perhaps developers using AI tools expanded the scope of the task: for instance, writing code to handle more edge cases, adding additional features, or testing or documenting code more thoroughly. As potential evidence in this direction, developers added 47% more lines of code (per forecasted task size [[3](https://secondthoughts.ai/p/ai-coding-slowdown/#footnote-3-167765295)] ) for AI Allowed tasks than AI Disallowed tasks. ([View Highlight](https://read.readwise.io/read/01jzzc48yre8p9p0r7sq0dpm08))

Based on exit interviews and analysis of screen recordings, the study authors identified several key sources of reduced productivity. The biggest issue is that the code generated by AI tools was generally not up to the high standards of these open-source projects. Developers spent substantial amounts of time reviewing the AI’s output, which often led to multiple rounds of prompting the AI, waiting for it to generate code, reviewing the code, discarding it as fatally flawed, and prompting the AI again. ([View Highlight](https://read.readwise.io/read/01jzzc6vb2f24aevr8t2dp3rq7))

